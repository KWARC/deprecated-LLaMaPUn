\documentclass{bluenote}
\usepackage{fullpage}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}

\title{Paragraph Discrimination using Bag of Words Model}
\author{Jan Frederik Schaefer}

\begin{document}
\maketitle

\begin{abstract}
A significant subset of the \emph{arXiv} documents has marked up paragraphs,
stating whether they are theorems, axioms, proofs etc.
We want to use these as a training set to be able to discriminate the remaining paragraphs.
In this sketch, I want to describe an approach which is similar to some spam detection techniques.
\end{abstract}

\section{The math}
%Our approach is to use the \emph{bag-of-words} model.
The idea of this approach is based on some concepts from \url{http://en.wikipedia.org/wiki/Bayesian_spam_filtering}.
We have a (small) set of paragraph types $T = \{\text{axiom}, \text{theorem}, \text{proof}, \ldots\}$.
We represent paragraphs as a set of the words $W$ it contains, ignoring their order or their number of occurences.
Now we are looking for the probability $P(T_i|W)$ that a paragraph with words $W$ is of type $T_i$.
Using Bayes' theorem, we get
$$P(T_i|W) = \frac{P(W|T_i)\cdot P(T_i)}{P(W)}$$
Apparently, we would get a bias towards the more common types $T_i$.
Probably, we should remove the bias by using a uniform distribution for $P(T_i)$.
Some authors didn't use the mark up consistently, and marked up e.g. only theorems,
so we might a wrong bias. Clearly, we have
$$P(W) = \sum_iP(W|T_i)\cdot P(T_i)$$
%The training set should be large enough to determine those probabilities.

Our goal is to determine $P(T_i|\mathbb{W})$, where $\mathbb{W}$ is the set of words contained in a paragraph.
There is a way to combine the probabilities of individual types of words
\footnote{see \url{http://en.wikipedia.org/wiki/Bayesian_spam_filtering\#Combining_individual_probabilities}}:
$$P(T_i|\mathbb{W}) = \frac{\prod_{w\in \mathbb{W}}P(T_i|w)}{\left(\prod_{w\in \mathbb{W}}P(T_i|w)\right)
\;+\; \prod_{w\in \mathbb{W}}\left(1-P(T_i|w)\right)}$$

\newpage

\subsection{Optimizing computation}
We have 
$$P(T_i|\mathbb{W}) = \frac{\prod_{w\in \mathbb{W}}P(T_i|w)}{\left(\prod_{w\in \mathbb{W}}P(T_i|w)\right)
\;+\; \prod_{w\in \mathbb{W}}\left(1-P(T_i|w)\right)}$$
$$\Leftrightarrow\quad\quad \frac{1}{P(T_i|\mathbb{W})}-1
= \frac{\prod_{w\in \mathbb{W}}\left(1-P(T_i|w)\right)}{\prod_{w\in \mathbb{W}}P(T_i|w)}$$
Additionally, we can use logarithms to turn the multiplication into addition:
$$\log\left(\frac{1}{P(T_i|\mathbb{W})}-1\right)
= \left(\sum_{w\in \mathbb{W}}\log\left(1-P(T_i|w)\right)\right)
-\left(\sum_{w\in \mathbb{W}}\log P(T_i|w)\right)$$



\section{Things we should consider}
\subsection{Normalization}
Since we have a limited set of marked up paragraphs, we might get better results if we stem words.
\subsection{Using only helpful words}
Some words might be more useful to consider than other words. E.g. the word \emph{the} wouldn't help us discriminating paragraphs,
whereas \emph{follows} might be very helpful. So ignoring the words which do not really hint at any type might yield clearer results.
\subsection{Skipping rare words}
Rare words can have devastating effects (strong noise, even division by zero problems)
\subsection{Using actual frequencies}
The math would get more complicated, but it might be very helpful to use the actual number of occurences of a word in a paragraph,
not just the binary \emph{is contained} vs. \emph{is not contained}.
\subsection{Using ngrams}
So far, we completely ignored the order of words. It might be worth to consider using bigrams instead of single words.
For larger ngrams the training set probably isn't large enough.



\end{document}